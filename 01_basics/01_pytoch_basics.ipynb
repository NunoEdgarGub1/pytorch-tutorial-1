{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic autograd example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Create tensors\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "# Build a computational graph\n",
    "y = w * x + b\n",
    "\n",
    "# Compute gradients\n",
    "y.backward()\n",
    "\n",
    "# Print the gradients\n",
    "print(x.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic autograd example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w Parameter containing:\n",
      "tensor([[-0.2043, -0.0936, -0.4224],\n",
      "        [ 0.2401, -0.1333,  0.3975]], requires_grad=True)\n",
      "b Parameter containing:\n",
      "tensor([-0.0187,  0.0294], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Create tensor of shape (10, 3) and (10, 2)\n",
    "x = torch.randn(10, 3)\n",
    "y = torch.randn(10, 2)\n",
    "\n",
    "# Build fully connected layer\n",
    "linear = nn.Linear(3, 2)\n",
    "print('w', linear.weight)\n",
    "print('b', linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8993, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Build loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
    "\n",
    "# Forward pass\n",
    "pred = linear(x)\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(pred, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dw tensor([[-0.1896, -0.1143, -0.0868],\n",
      "        [-0.1112, -0.7044,  0.4858]])\n",
      "dL/db tensor([-0.4438,  0.0749])\n"
     ]
    }
   ],
   "source": [
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "print('dL/dw', linear.weight.grad)\n",
    "print('dL/db', linear.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-step gradient descent\n",
    "# This is equivalent to:\n",
    "# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 1-step optimization: 0.8892849683761597\n"
     ]
    }
   ],
   "source": [
    "# Print loss after 1-step gradient descent\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('Loss after 1-step optimization:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data from numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numpy array\n",
    "x = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "# Convert the numpy array to a torch tensor\n",
    "y = torch.from_numpy(x)\n",
    "\n",
    "# Covert the torch tensor to a numpy array\n",
    "z = y.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 169058304/170498071 [00:16<00:00, 12069812.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "torch.Size([3, 32, 32])\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "170500096it [00:29, 12069812.06it/s]                               "
     ]
    }
   ],
   "source": [
    "# Download and construct CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', \n",
    "                                             train=True, \n",
    "                                             transform=transforms.ToTensor(), \n",
    "                                             download=True)\n",
    "\n",
    "# Fetch one data pair (read from disk)\n",
    "image, label = train_dataset[0]\n",
    "print(image.size())\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader (this provides queues and threads in a very simple way)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# When iteration starts, queue and thread start to load data from files\n",
    "data_iter = iter(train_loader)\n",
    "\n",
    "# Mini-batch images and labels\n",
    "image, label = data_iter.next()\n",
    "\n",
    "# Actual usage of the data loader is as below\n",
    "for images, labels in train_loader:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input pipline for custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-77d28845f30f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n\u001b[1;32m     20\u001b[0m                                            \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                                            shuffle=True)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0;32m---> 94\u001b[0;31m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "# You should build your custom dataset as below\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        # TODO\n",
    "        # 1. Initialize file paths or a list of file names\n",
    "        pass\n",
    "    def __getitem__(self, index):\n",
    "        # TODO\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open)\n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform)\n",
    "        # 3. Return a data pair (e.g. image and label)\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        # You should change 0 to the total size of your dataset\n",
    "        return 0\n",
    "    \n",
    "# You can then use the prebuilt data loader\n",
    "custom_dataset = CustomDataset()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /hpc/home/ephyan/.cache/torch/checkpoints/resnet18-5c106cde.pth\n",
      "\n",
      "  0%|          | 0.00/44.7M [00:00<?, ?B/s]\u001b[A\n",
      "  3%|▎         | 1.46M/44.7M [00:00<00:02, 15.3MB/s]\u001b[A\n",
      "  9%|▊         | 3.88M/44.7M [00:00<00:02, 17.3MB/s]\u001b[A\n",
      " 11%|█         | 4.92M/44.7M [00:00<00:03, 13.3MB/s]\u001b[A\n",
      " 17%|█▋        | 7.63M/44.7M [00:00<00:02, 15.8MB/s]\u001b[A\n",
      " 21%|██        | 9.44M/44.7M [00:00<00:02, 16.6MB/s]\u001b[A\n",
      " 25%|██▍       | 11.1M/44.7M [00:00<00:02, 16.7MB/s]\u001b[A\n",
      " 28%|██▊       | 12.7M/44.7M [00:00<00:01, 16.9MB/s]\u001b[A\n",
      " 35%|███▌      | 15.6M/44.7M [00:00<00:01, 19.5MB/s]\u001b[A\n",
      " 40%|███▉      | 17.8M/44.7M [00:00<00:01, 20.2MB/s]\u001b[A\n",
      " 47%|████▋     | 20.8M/44.7M [00:01<00:01, 22.6MB/s]\u001b[A\n",
      " 52%|█████▏    | 23.1M/44.7M [00:01<00:01, 20.8MB/s]\u001b[A\n",
      " 59%|█████▉    | 26.2M/44.7M [00:01<00:00, 23.2MB/s]\u001b[A\n",
      " 64%|██████▍   | 28.7M/44.7M [00:01<00:00, 18.3MB/s]\u001b[A\n",
      " 69%|██████▉   | 30.7M/44.7M [00:01<00:00, 16.8MB/s]\u001b[A\n",
      " 73%|███████▎  | 32.5M/44.7M [00:01<00:00, 15.5MB/s]\u001b[A\n",
      " 77%|███████▋  | 34.2M/44.7M [00:01<00:00, 13.8MB/s]\u001b[A\n",
      " 81%|████████  | 36.2M/44.7M [00:02<00:00, 15.4MB/s]\u001b[A\n",
      " 87%|████████▋ | 38.7M/44.7M [00:02<00:00, 17.5MB/s]\u001b[A\n",
      " 91%|█████████ | 40.6M/44.7M [00:02<00:00, 17.6MB/s]\u001b[A\n",
      " 95%|█████████▍| 42.4M/44.7M [00:02<00:00, 16.3MB/s]\u001b[A\n",
      "100%|██████████| 44.7M/44.7M [00:02<00:00, 19.1MB/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100])\n"
     ]
    }
   ],
   "source": [
    "# Download and load the pretrained ResNet-18\n",
    "resnet = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# If you want to finetune only the top layer of the model, set as below\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False  # Freeze the params\n",
    "    \n",
    "# Replace the top layer for finetuning\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, 100)  # 100 is an exmaple\n",
    "\n",
    "# Forward pass\n",
    "images = torch.randn(64, 3, 224, 224)  # Generate 64 224*224 images with 3 channels\n",
    "outputs = resnet(images)\n",
    "print(outputs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save and load the entire model\n",
    "torch.save(resnet, 'model.ckpt')\n",
    "model = torch.load('model.ckpt')\n",
    "\n",
    "# Save and load only the model parameters (recommended)\n",
    "torch.save(resnet.state_dict(), 'params.ckpt')\n",
    "resnet.load_state_dict(torch.load('params.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
